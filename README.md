# machine_learning2

本阶段涉及内容如下

朴素贝叶斯
支持向量机(SVM)
EM算法
HMM模型

## [朴素贝叶斯](https://zsyll.blog.csdn.net/article/details/119062299)

基于概率进行预测 ,算法输出结果为样本属于某个类别的概率值



### 联合概率、条件概率与相互独立

- 联合概率：包含多个条件，且所有条件同时成立的概率
  - 记作：P(A,B)
- 条件概率：就是事件A在另外⼀个事件B已经发生条件下的发生概率
  - 记作：P(A|B)
- 相互独立：如果P(A, B) = P(A)P(B)，则称事件A与事件B相互独立。

#### 贝叶斯公式

$$
P(C|W) = \frac{P(W|C)P(C)}{P(W)}\\
注：w为给定文档的特征值（频数统计，预测文档提供），c为文档类别
$$

通过上述式子,可以得到给出文档的文档类别

- **朴素贝叶斯**，简单理解，就是假定了**特征与特征之间相互独立**的贝叶斯公式。

- P(C)：每个文档类别的概率(某文档类别数／总文档数量)
- P(W│C)：给定类别下特征（被预测文档中出现的词）的概率
- 计算方法：P(F1│C)=Ni/N （训练⽂档中去计算）
  - Ni为该F1词在C类别所有文档中出现的次数
  - N为所属类别C下的文档所有词出现的次数和
- P(F1,F2,…) 预测文档中每个词的概率



#### 例1.

 2.4.3 文章分类计算

**需求**：通过前四个训练样本（文章），判断第五篇文章，是否属于China类

|        | 文档ID | 文档中的词                          | 属于c=China类 |
| ------ | ------ | ----------------------------------- | ------------- |
| 训练集 | 1      | Chinese Beijing Chinese             | Yes           |
|        | 2      | Chinese Chinese Shanghai            | Yes           |
|        | 3      | Chinese Macao                       | Yes           |
|        | 4      | Tokyo Japan Chinese                 | No            |
| 测试集 | 5      | Chinese Chinese Chinese Tokyo Japan | ？            |

从上面的例子我们得到P(TokvolC)和P(JapanlC)都为O，这是不合理的，如果词频列表里面有很多出现次数都为0，很可能计算结果都为零。

##### 词频为0解决方法：拉普拉斯平滑系数

$$ P(F1|C) = \frac{Ni + \alpha}{N + \alpha m} $$

α为系数一般为1,m为训练文档中统计出的特征词总数
$$
P(C|Chinese, Chinese, Chinese, Tokyo, Japan) -->\\
P(Chinese, Chinese, Chinese, Tokyo, Japan|C) * P(C) / \\
P(Chinese, Chinese, Chinese, Tokyo, Japan) \\
=P^3(Chinese|C) * P(Tokyo|C) * P(Japan|C) * P(C) / \\
[P^3(Chinese) * P(Tokyo) * P(Japan)]
$$


```
公式如上
# 这个部分是需要计算是不是China类:

⾸先计算是China类的概率: 0.0003 
	P(Chinese|C) = 5/8 --> 6/14  (5+1)/(8+6) 下同 
	P(Tokyo|C) = 0/8 --> 1/14 
	P(Japan|C) = 0/8 --> 1/14

接着计算不是China类的概率: 0.0001 
	P(Chinese|C) = 1/3 -->(经过拉普拉斯平滑系数处理) 2/9 
	P(Tokyo|C) = 1/3 --> 2/9 
	P(Japan|C) = 1/3 --> 2/9
```

因此最后结果是China类



###  3. 案例：商品评论情感分析



#### 3.1 API介绍

- sklearn.naive_bayes.MultinomialNB(alpha = 1.0)
  - 朴素贝叶斯分类
  - **alpha**：拉普拉斯平滑系数

```python
data.loc[data.loc[:, '评价'] == '好评', '评论编号'] = 1
data.loc[data.loc[:, '评价'] == '差评', '评论编号'] = 0
```

上述代码:加一列'评论编号',如果'评价'=='好评',则'评论编号'的值为1;=='差评', '评论编号'的值为0

### 4. 朴素贝叶斯算法总结

#### 4.1 朴素贝叶斯优缺点

##### 优点：

朴素贝叶斯模型发源于古典数学理论，有**稳定**的分类效率
对**缺失数据不太敏感**，**算法简单**，常用于文本分类
分类**准确度高，速度快**

##### 缺点：

由于使用了**样本属性独立性**的假设，所以如果**特征属性有关联时其效果不好**
需要计算先验概率，而**先验概率很多时候取决于假设**，假设的模型可以有很多种，因此在某些时候会**由于假设的先验模型的原因导致预测效果不佳**；

##### 4.2.4 在估计条件概率P(X∣Y)时出现概率为0的情况怎么办？

解决这⼀问题的方法是采用贝叶斯估计, 即引入 λ

当**λ=0**时，就是普通的**极大似然估计**；
当**λ=1**时称为**拉普拉斯平滑**。

##### 4.2.5 为什么属性独立性假设在实际情况中很难成立，但朴素贝叶斯仍能取得较好的效果?

人们在使用分类器之前，首先做的第⼀步（也是最重要的⼀步）往往是特征选择，这个过程的目的就是为了**排除特征之间的共线性、选择相对较为独立的特征**；
对于分类任务来说，只要各类别的条件概率排序正确，无需精准概率值就可以得出正确分类；
**如果属性间依赖对所有类别影响相同，或依赖关系的影响能相互抵消，则属性条件独立性假设在降低计算复杂度的同时不会对性能产生负面影响。**

##### 4.2.6 朴素贝叶斯与LR(逻辑回归)的区别？

1）简单来说：

###### 区别⼀：

朴素贝叶斯是生成模型，
根据已有样本进行贝叶斯估计学习出先验概率P(Y)和条件概率P(X|Y)，
进而求出联合分布概率P(XY), 最后利用贝叶斯定理求解P(Y|X)，
而**LR是判别模型，**
**根据极大化对数似然函数直接求出条件概率P(Y|X)**；
从概率框架的⻆度来理解机器学习；主要有两种策略：
第⼀种：**给定 x， 可通过直接建模 P(c |x) 来预测 c，这样得到的是"判别式模型"** **(discriminative models)；**
第⼆种：**也可先对联合概率分布 P(x,c) 建模，然后再由此获得 P(c |x)， 这样得到的是"生成式模型"** **(generative models)** ;
显然，前⾯介绍的**逻辑回归、决策树、都可归入判别式模型**的范畴，还有后⾯学到的BP神经网络⽀持向量机等；

###### 区别二：

**朴素贝叶斯**是基于很强的**条件独立假设**（在已知分类Y的条件下，各个特征变量取值是相互独立的），
而**LR**则对此**没有要求**；

###### 区别三：

**朴素贝叶斯**适用于**数据集少**的情景，
而**LR**适用于**大规模数据集**。

**生成式模型和判别式模型的区别**

###### 生成式模型 (如朴素贝叶斯) :试图找出函数f

首先，Navie Bayes通过已知样本求得先验概率P(Y), 及条件概率P(X|Y), 对于给定的实例，计算联合概率，进而求出后验概率。也就是说，**它尝试去找到底这个数据是怎么生成的**（产⽣的），然后再进行分类。哪个类别最有可能产生这个信号，就属于那个类别。

优点： **样本容量增加时，收敛更快；隐变量存在时也可适用**。
缺点：**时间长；需要样本多；浪费计算资源**

###### 判别式模型 (如逻辑回归)  用已有数据修正拟合式 f

相比之下，**Logistic回归不关心样本中类别的比例及类别下出现特征的概率**，它**直接给出预测模型的形式**。设每个特征都有⼀个权重，**训练样本数据更新权重w**，得出最终表达式。

优点
直接预测往往**准确率更高**；
**简化问题**；
可以**反应数据的分布情况，类别的差异特征**；
**适用于较多类别的识别**。
缺点
**收敛慢；**
**不适用于有隐变量的情况**。











## 支持向量机(SVM)

#### 1. 定义

**寻找到⼀个超平⾯使样本分成两类，并且间隔最⼤。**

![在这里插入图片描述](README.assets/0b32772d9819c0df30a8376e6e8f1863.png)

左图虚线表现显然不行,红粉两线离边界太近,效果不好。右图显然中间的实线表现最好

### 2 硬间隔和软间隔

##### 2.1 硬间隔

在上⾯我们使⽤超平⾯进⾏分割数据的过程中，如果我们**严格地让所有实例都不在最⼤间隔之间**，并且位于正确的一边，这就是硬间隔分类。

缺点:**硬间隔**只在**数据是线性可分离的时候才有效**；其次，**它对异常值非常敏感**

![image-20250306233642499](README.assets/image-20250306233642499.png)

如上图,异常值导致难以找出合适的分割点

##### 2.2 软间隔

要避免这些问题，最好使⽤**更灵活的模型**。⽬标是尽可能在保**持最大间隔宽阔和限制间隔违例**（即位于最⼤间隔之上， 甚⾄在错误的⼀边的实例）之间找到良好的平衡，这就是软间隔分类。

在Scikit-Learn的SVM类中，可以通过**超参数C来控制这个平衡：C值越⼩，则间隔越宽，但是间隔违例也会越多**。

#### 3.SVM算法原理

T = {(x₁, y₁), (x₂, y₂), ... (xₙ, yₙ)}

xᵢ ∈ Rⁿ, yᵢ ∈ {+1, -1}, i = 1, 2, ... N₀₆₁

其中，(xᵢ, yᵢ) 称为样本点。

- xᵢ为第i个实例（样本），
- yᵢ为xᵢ的标记：
  - 当 yᵢ = 1时，xᵢ为正例
  - 当 yᵢ = -1时，xᵢ为负例

##### 3.2 线性可分支持向量机

原分割函数为 f(x)=w^T  x+b

给定了上面提出的线性可分训练数据集，通过**间隔最大化得到分离超平面**为：y(x)=wΦ(x)+b
相应的分类决策函数为：f(x)=sign(wΦ(x)+b)
以上**决策函数就称为线性可分支持向量机**。

Φ(x) 是某个确定的特征空间转换函数，它的作用是**将x映射到更高的维度**，它有一个以后我们经常会见到的专有称号”**核函数**“。

<img src="README.assets/9bcb0671e53a6743432ba06e97b87a06.png" alt="在这里插入图片描述" style="zoom:50%;" />

##### 3.3 SVM的计算过程与算法步骤

###### 3.3.1 推导目标函数

我们知道了⽀持向量机是个什么东⻄了。现在我们要去寻找这个⽀持向量机，也就是寻找⼀个最优的超平⾯。

于是我们要建⽴⼀个⽬标函数。那么如何建⽴呢？

样本空间中任意点到超平面(w,b)的距离可写成$$r = \frac{|{w}^T {x} + b|}{||{w}||}$$
$$
\begin{cases}
\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x}_{i} + b \geqslant +1, & y_{i} = +1 \\
\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x}_{i} + b \leqslant -1, & y_{i} = -1
\end{cases}
$$
如图所示，距离超平面**最近的几个训练样本点**使上式**等号成立**，**他们被称为“支持向量”**，

两个**异类支持向量到超平面的距离之和**为：$$\gamma = \frac{2}{\|w\|}$$

![image-20250307002616832](README.assets/image-20250307002616832.png)

因此我们要求满足$$\begin{cases}
\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x}_{i} + b \geqslant +1, & y_{i} = +1 \\
\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x}_{i} + b \leqslant -1, & y_{i} = -1
\end{cases}$$的*w*和*b*,使得γ最大,即$$max_{w,b}\frac{2}{\|w\|}$$,其中$$ y_i \left( w^T x_i + b \right) \geq 1, i = 1, 2, \cdots, m $$ (各项预测均正确) ,故最大化$$\frac{1}{\|w\|}$$等价于最小化$$||w||^2$$
支持向量机的目标函数可以视为$$min_{x,b} \frac{1}{2}||w||^2$$ 1/2用于求导时便于计算



###### 3.3.2 目标函数的求解

⽬标函数带有⼀个约束条件，所以我们可以⽤**拉格朗日乘子法**,寻找多元函数在约束下的极值.

L=f+λg 其中g(x)=0是求解f最值的约束条件

经过朗格朗日乘子法，我们可以把目标函数转换为：

$$L(w, b, \alpha) = \frac{1}{2} \|w\|^2 + \sum_{i=1}^{n} \alpha_i (1 - y_i (w^T \cdot \Phi(x_i) + b) )$$

其中，要想求得极小值，上式后半部分：

$$\sum_{i=1}^{n} \alpha_i (y_i (w^T \cdot \Phi(x_i) + b) - 1) = 0$$,这是之前的条件得到的

走到这一步，这个目标函数还是不能开始求解，**现在我们的问题是极小极大值问题**。

![img](README.assets/4b8e605f68bbb47db0895d685e2d0cc6.png)

建议手写一下,这部分就是求偏导然后代回原式

# TODO

[链接](https://zsyll.blog.csdn.net/category_10993525.html)
[链接](https://blog.csdn.net/2201_75415080?type=blog) 



## EM算法



## HMM模型

## 
